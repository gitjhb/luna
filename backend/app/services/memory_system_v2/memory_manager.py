"""
Memory System v2 - åˆ†å±‚è®°å¿†ç³»ç»Ÿ
==============================

ä¸‰å±‚æ¶æ„ï¼š
1. å·¥ä½œè®°å¿† (Working Memory) - å½“å‰å¯¹è¯ä¸Šä¸‹æ–‡
2. æƒ…èŠ‚è®°å¿† (Episodic Memory) - é‡è¦äº‹ä»¶å’Œå¯¹è¯
3. è¯­ä¹‰è®°å¿† (Semantic Memory) - ç”¨æˆ·ç‰¹å¾å’Œåå¥½

è®© AI è§’è‰²çœŸæ­£"è®°ä½"ç”¨æˆ·
"""

import asyncio
import logging
import json
import re
from datetime import datetime, timedelta
from typing import Optional, Dict, Any, List, Tuple
from dataclasses import dataclass, field
from enum import Enum
import hashlib

logger = logging.getLogger(__name__)


# =============================================================================
# æ•°æ®ç»“æ„å®šä¹‰
# =============================================================================

class MemoryType(Enum):
    """è®°å¿†ç±»å‹"""
    EPISODIC = "episodic"    # æƒ…èŠ‚è®°å¿†ï¼ˆäº‹ä»¶ï¼‰
    SEMANTIC = "semantic"     # è¯­ä¹‰è®°å¿†ï¼ˆç‰¹å¾ï¼‰
    EMOTIONAL = "emotional"   # æƒ…æ„Ÿè®°å¿†ï¼ˆæƒ…ç»ªé«˜ç‚¹/ä½ç‚¹ï¼‰


class MemoryImportance(Enum):
    """è®°å¿†é‡è¦æ€§"""
    LOW = 1       # æ™®é€šå¯¹è¯
    MEDIUM = 2    # æœ‰ä»·å€¼ä¿¡æ¯
    HIGH = 3      # é‡è¦äº‹ä»¶
    CRITICAL = 4  # é‡Œç¨‹ç¢‘æ—¶åˆ»


@dataclass
class EpisodicMemory:
    """æƒ…èŠ‚è®°å¿† - ä¸€ä¸ªå…·ä½“äº‹ä»¶"""
    memory_id: str
    user_id: str
    character_id: str
    
    # å†…å®¹
    event_type: str          # first_meeting / confession / fight / gift / milestone
    summary: str             # äº‹ä»¶æ‘˜è¦
    key_dialogue: List[str]  # å…³é”®å¯¹è¯ï¼ˆæœ€å¤š3å¥ï¼‰
    emotion_state: str       # å½“æ—¶çš„æƒ…ç»ªçŠ¶æ€
    
    # å…ƒæ•°æ®
    importance: MemoryImportance
    created_at: datetime
    last_recalled: Optional[datetime] = None
    recall_count: int = 0
    
    # è¡°å‡
    strength: float = 1.0    # è®°å¿†å¼ºåº¦ 0.0-1.0ï¼Œä¼šéšæ—¶é—´è¡°å‡
    
    def to_prompt_text(self) -> str:
        """è½¬æ¢ä¸º prompt å¯ç”¨çš„æ–‡æœ¬"""
        dialogue_text = "\n".join([f'  "{d}"' for d in self.key_dialogue[:2]])
        return f"[{self.event_type}] {self.summary}\n{dialogue_text}"


@dataclass
class SemanticMemory:
    """è¯­ä¹‰è®°å¿† - ç”¨æˆ·ç‰¹å¾"""
    user_id: str
    character_id: str
    
    # åŸºæœ¬ä¿¡æ¯
    user_name: Optional[str] = None
    user_nickname: Optional[str] = None  # ç”¨æˆ·å¸Œæœ›è¢«å«çš„æ˜µç§°
    birthday: Optional[str] = None
    occupation: Optional[str] = None
    location: Optional[str] = None
    
    # åå¥½
    likes: List[str] = field(default_factory=list)      # å–œæ¬¢çš„ä¸œè¥¿
    dislikes: List[str] = field(default_factory=list)   # è®¨åŒçš„ä¸œè¥¿
    interests: List[str] = field(default_factory=list)  # å…´è¶£çˆ±å¥½
    
    # æ€§æ ¼ç‰¹å¾ï¼ˆAI è§‚å¯Ÿåˆ°çš„ï¼‰
    personality_traits: List[str] = field(default_factory=list)
    communication_style: Optional[str] = None  # æ²Ÿé€šé£æ ¼
    
    # å…³ç³»ç›¸å…³
    relationship_status: Optional[str] = None  # å•èº«/æ‹çˆ±ä¸­
    pet_names: List[str] = field(default_factory=list)  # äº’ç›¸çš„æ˜µç§°
    important_dates: Dict[str, str] = field(default_factory=dict)  # çºªå¿µæ—¥ç­‰
    shared_jokes: List[str] = field(default_factory=list)  # å…±åŒçš„æ¢—
    
    # æ•æ„Ÿè¯é¢˜
    sensitive_topics: List[str] = field(default_factory=list)  # é¿å…æåŠçš„è¯é¢˜
    
    # æ›´æ–°æ—¶é—´
    updated_at: datetime = field(default_factory=datetime.now)
    
    def to_prompt_text(self) -> str:
        """è½¬æ¢ä¸º prompt å¯ç”¨çš„æ–‡æœ¬"""
        parts = []
        
        if self.user_name:
            parts.append(f"ç”¨æˆ·åå­—: {self.user_name}")
        if self.user_nickname:
            parts.append(f"ç”¨æˆ·å–œæ¬¢è¢«å«: {self.user_nickname}")
        if self.birthday:
            parts.append(f"ç”Ÿæ—¥: {self.birthday}")
        if self.occupation:
            parts.append(f"èŒä¸š: {self.occupation}")
        if self.likes:
            parts.append(f"å–œæ¬¢: {', '.join(self.likes[:5])}")
        if self.dislikes:
            parts.append(f"ä¸å–œæ¬¢: {', '.join(self.dislikes[:5])}")
        if self.interests:
            parts.append(f"å…´è¶£: {', '.join(self.interests[:5])}")
        if self.pet_names:
            parts.append(f"ä½ ä»¬ä¹‹é—´çš„æ˜µç§°: {', '.join(self.pet_names[:3])}")
        if self.important_dates:
            dates = [f"{k}: {v}" for k, v in list(self.important_dates.items())[:3]]
            parts.append(f"é‡è¦æ—¥æœŸ: {', '.join(dates)}")
        if self.sensitive_topics:
            parts.append(f"é¿å…æåŠ: {', '.join(self.sensitive_topics[:3])}")
        
        return "\n".join(parts)


@dataclass
class MemoryContext:
    """è®°å¿†ä¸Šä¸‹æ–‡ - ä¼ é€’ç»™ LLM çš„è®°å¿†ä¿¡æ¯"""
    # å·¥ä½œè®°å¿†
    working_memory: List[Dict[str, str]]  # æœ€è¿‘çš„å¯¹è¯
    
    # æƒ…èŠ‚è®°å¿†
    relevant_episodes: List[EpisodicMemory]  # ç›¸å…³çš„äº‹ä»¶
    recent_episodes: List[EpisodicMemory]    # æœ€è¿‘çš„äº‹ä»¶
    
    # è¯­ä¹‰è®°å¿†
    user_profile: SemanticMemory
    
    # ä»Šæ—¥ç‰¹æ®Š
    today_special: Optional[str] = None  # ä»Šå¤©æ˜¯å¦æ˜¯ç‰¹æ®Šæ—¥å­
    
    # çº¦ä¼š/äº‹ä»¶è®°å¿†ï¼ˆä» EventMemory è¡¨ï¼‰
    event_memories: List[Dict[str, Any]] = field(default_factory=list)
    
    def to_prompt_section(self) -> str:
        """ç”Ÿæˆå®Œæ•´çš„è®°å¿† prompt éƒ¨åˆ†"""
        sections = []
        
        # ç”¨æˆ·æ¡£æ¡ˆ
        if self.user_profile:
            profile_text = self.user_profile.to_prompt_text()
            if profile_text:
                sections.append(f"=== å…³äºç”¨æˆ· ===\n{profile_text}")
        
        # ä»Šæ—¥ç‰¹æ®Š
        if self.today_special:
            sections.append(f"â­ ä»Šå¤©ç‰¹æ®Š: {self.today_special}")
        
        # å…±åŒå›å¿†
        if self.relevant_episodes or self.recent_episodes:
            memory_lines = []
            
            # ç›¸å…³è®°å¿†ï¼ˆæ ¹æ®å½“å‰è¯é¢˜æ£€ç´¢ï¼‰
            for ep in self.relevant_episodes[:3]:
                memory_lines.append(f"â€¢ {ep.summary}")
            
            # æœ€è¿‘è®°å¿†
            for ep in self.recent_episodes[:2]:
                if ep not in self.relevant_episodes:
                    memory_lines.append(f"â€¢ (æœ€è¿‘) {ep.summary}")
            
            if memory_lines:
                sections.append(f"=== ä½ ä»¬çš„å›å¿† ===\n" + "\n".join(memory_lines))
        
        # çº¦ä¼š/äº‹ä»¶è®°å¿†ï¼ˆä» EventMemory è¡¨ï¼‰
        if self.event_memories:
            event_lines = []
            for event in self.event_memories[:5]:  # æœ€å¤šæ˜¾ç¤º5ä¸ª
                event_type = event.get("event_type", "")
                summary = event.get("context_summary", "") or event.get("story_content", "")[:100]
                if event_type == "date":
                    event_lines.append(f"ğŸ’• çº¦ä¼š: {summary}")
                elif event_type == "first_date":
                    event_lines.append(f"ğŸ’ ç¬¬ä¸€æ¬¡çº¦ä¼š: {summary}")
                elif event_type == "gift":
                    event_lines.append(f"ğŸ æ”¶åˆ°ç¤¼ç‰©: {summary}")
                else:
                    event_lines.append(f"ğŸ“Œ {event_type}: {summary}")
            
            if event_lines:
                sections.append(f"=== é‡è¦äº‹ä»¶ ===\n" + "\n".join(event_lines))
        
        return "\n\n".join(sections)


# =============================================================================
# è®°å¿†æå–å™¨ - ä»å¯¹è¯ä¸­æå–è®°å¿†
# =============================================================================

class MemoryExtractor:
    """
    è®°å¿†æå–å™¨ v2 - "éšå½¢åœºè®°"æ¨¡å¼
    
    ä½¿ç”¨ LLM è¯­ä¹‰ç†è§£ä»å¯¹è¯ä¸­æå–ï¼š
    1. ç”¨æˆ·ä¿¡æ¯ï¼ˆè¯­ä¹‰è®°å¿†ï¼‰
    2. é‡è¦äº‹ä»¶ï¼ˆæƒ…èŠ‚è®°å¿†ï¼‰
    
    æ ¸å¿ƒä¼˜åŠ¿ï¼š
    - è·¨è¯­è¨€ï¼šä¸­/è‹±/æ—¥/æ³•...ç»Ÿä¸€å¤„ç†
    - è¯­å¢ƒæ„ŸçŸ¥ï¼šèƒ½åŒºåˆ†"äº²äº†"vs"æƒ³äº²ä½†è¢«æ‹’"vs"æ¢¦åˆ°äº²äº†"
    - å¦å®šå¥è¯†åˆ«ï¼š"åˆ«äº²æˆ‘"ä¸ä¼šè¯¯åˆ¤ä¸ºäº²å¯†äº‹ä»¶
    """
    
    # è½»é‡çº§é¢„ç­›é€‰å…³é”®è¯ï¼ˆåªç”¨äºå†³å®šæ˜¯å¦éœ€è¦è°ƒç”¨LLMåœºè®°ï¼‰
    # å¦‚æœæ¶ˆæ¯é‡Œæ²¡æœ‰è¿™äº›è¯ï¼Œå¤§æ¦‚ç‡æ˜¯é—²èŠï¼Œå¯ä»¥è·³è¿‡åœºè®°
    POTENTIAL_EVENT_HINTS = [
        # äº²å¯†è¡Œä¸º
        "äº²", "å»", "kiss", "æŠ±", "hug", "ç‰µæ‰‹", "hold", "æ‘¸",
        # æƒ…æ„Ÿè¡¨è¾¾
        "çˆ±", "å–œæ¬¢", "love", "è®¨åŒ", "hate", "ç”Ÿæ°”", "angry", "æƒ³ä½ ", "miss",
        # å…³ç³»å˜åŒ–
        "åˆ†æ‰‹", "break", "åœ¨ä¸€èµ·", "together", "ç»“å©š", "marry", "æ±‚å©š", "propose",
        "å¥³æœ‹å‹", "ç”·æœ‹å‹", "girlfriend", "boyfriend", "è€å©†", "è€å…¬", "wife", "husband",
        # é‡è¦æ—¶åˆ»
        "ç¬¬ä¸€æ¬¡", "first", "çºªå¿µ", "anniversary", "ç”Ÿæ—¥", "birthday",
        # ç¤¼ç‰©/æƒŠå–œ
        "ç¤¼ç‰©", "gift", "é€ä½ ", "æƒŠå–œ", "surprise",
        # é“æ­‰/å’Œè§£
        "å¯¹ä¸èµ·", "sorry", "åŸè°…", "forgive", "æˆ‘é”™äº†",
        # æƒ…ç»ªé«˜ç‚¹
        "æœ€å¼€å¿ƒ", "æœ€å¹¸ç¦", "æœ€éš¾è¿‡", "happiest", "saddest",
    ]
    
    def __init__(self, llm_service=None):
        self.llm = llm_service
    
    def _needs_scene_analysis(self, message: str, assistant_response: str = "") -> bool:
        """
        å¿«é€Ÿé¢„ç­›é€‰ï¼šåˆ¤æ–­æ˜¯å¦éœ€è¦è°ƒç”¨ LLM åœºè®°
        
        ç­–ç•¥ï¼šå¦‚æœæ¶ˆæ¯æˆ–å›å¤é‡ŒåŒ…å«æ½œåœ¨äº‹ä»¶å…³é”®è¯ï¼Œæ‰è°ƒç”¨åœºè®°
        è¿™æ ·å¯ä»¥èŠ‚çœ 90% çš„é—²èŠæ¶ˆæ¯çš„ LLM è°ƒç”¨
        """
        combined = (message + " " + (assistant_response or "")).lower()
        return any(hint in combined for hint in self.POTENTIAL_EVENT_HINTS)
    
    async def extract_from_message(
        self,
        message: str,
        context: List[Dict[str, str]],
        current_semantic: SemanticMemory,
        assistant_response: str = "",
    ) -> Tuple[Optional[Dict[str, Any]], Optional[Dict[str, Any]]]:
        """
        ä»æ¶ˆæ¯ä¸­æå–è®°å¿†ï¼ˆåœºè®°æ¨¡å¼ï¼‰
        
        è¿”å›:
            (semantic_updates, episodic_event)
        """
        # å¿«é€Ÿé¢„ç­›é€‰
        if not self._needs_scene_analysis(message, assistant_response):
            logger.debug(f"Skipping scene analysis for: {message[:50]}...")
            return {}, None
        
        # è°ƒç”¨ LLM åœºè®°
        if not self.llm:
            logger.warning("No LLM service available for scene analysis")
            return {}, None
        
        try:
            result = await self._scene_supervisor_extract(
                message, assistant_response, context, current_semantic
            )
            return result.get("semantic", {}), result.get("episodic")
        except Exception as e:
            logger.warning(f"Scene analysis failed: {e}")
            return {}, None
    
    async def _scene_supervisor_extract(
        self,
        user_message: str,
        assistant_response: str,
        context: List[Dict[str, str]],
        current_semantic: SemanticMemory,
    ) -> Dict[str, Any]:
        """
        LLM åœºè®°åˆ†æ - è¯­ä¹‰çº§è®°å¿†æå–
        
        è¿™æ˜¯æ ¸å¿ƒçš„"éšå½¢åœºè®°"ï¼Œè´Ÿè´£ï¼š
        1. åˆ¤æ–­æ˜¯å¦å‘ç”Ÿäº†é‡è¦äº‹ä»¶
        2. æå–ç”¨æˆ·ä¿¡æ¯æ›´æ–°
        3. åŒºåˆ†çœŸå®äº‹ä»¶ vs æ¢¦å¢ƒ/å‡è®¾/å¦å®š
        """
        # æ„å»ºå¯¹è¯ä¸Šä¸‹æ–‡
        dialogue_lines = []
        for m in context[-3:]:
            role = "ç”¨æˆ·" if m.get("role") == "user" else "Luna"
            dialogue_lines.append(f"{role}: {m.get('content', '')[:150]}")
        dialogue_lines.append(f"ç”¨æˆ·: {user_message}")
        if assistant_response:
            dialogue_lines.append(f"Luna: {assistant_response[:200]}")
        
        dialogue_str = "\n".join(dialogue_lines)
        
        current_info = current_semantic.to_prompt_text() if current_semantic else "æ— å·²çŸ¥ä¿¡æ¯"
        
        prompt = f"""# Role
ä½ æ˜¯ Luna æ‹çˆ±æ¸¸æˆçš„åå°å‰§æƒ…åˆ†æå¸ˆï¼ˆåœºè®°ï¼‰ã€‚

# å·²çŸ¥ç”¨æˆ·ä¿¡æ¯
{current_info}

# æœ¬è½®å¯¹è¯
{dialogue_str}

# Task
åˆ†æä»¥ä¸Šå¯¹è¯ï¼Œæå–ï¼š
1. **ç”¨æˆ·ä¿¡æ¯æ›´æ–°**ï¼ˆåå­—ã€ç”Ÿæ—¥ã€èŒä¸šã€å–œå¥½ç­‰æ–°ä¿¡æ¯ï¼‰
2. **é‡è¦äº‹ä»¶**ï¼ˆå¦‚æœæœ‰çš„è¯ï¼‰

# é‡è¦è§„åˆ™
- åªè®°å½•**å®é™…å‘ç”Ÿ**çš„äº‹ä»¶ï¼Œä¸è®°å½•"æƒ³è¦ä½†æ²¡å‘ç”Ÿ"çš„
- æ¢¦å¢ƒã€å›å¿†ã€å‡è®¾ã€å¦å®šå¥ = ä¸ç®—å®é™…å‘ç”Ÿ
- "äº²æˆ‘" + "ä¸è¦" = rejectionï¼ˆæ±‚æ¬¢è¢«æ‹’ï¼‰ï¼Œä¸æ˜¯ intimate
- "äº²æˆ‘" + "*äº²äº†*" = intimateï¼ˆå®é™…å‘ç”Ÿï¼‰
- è·¨è¯­è¨€ç»Ÿä¸€ï¼šæ— è®ºä¸­è‹±æ—¥æ³•ï¼Œè¾“å‡ºæ ‡å‡†åŒ–å­—æ®µ

# Event Types
- confession: è¡¨ç™½ã€è¯´"æˆ‘çˆ±ä½ "
- intimate: äº²å»ã€æ‹¥æŠ±ã€ç‰µæ‰‹ç­‰å®é™…å‘ç”Ÿçš„äº²å¯†è¡Œä¸º
- rejection: ä¸€æ–¹æ±‚äº²å¯†/æ±‚çˆ±è¢«æ‹’ç»
- fight: åµæ¶ã€å†·æˆ˜ã€è¯´ç‹ è¯
- reconciliation: é“æ­‰ã€å’Œå¥½
- milestone: ç¬¬ä¸€æ¬¡çº¦ä¼šã€ä¸€å‘¨å¹´ã€ä¸€ç™¾å¤©ç­‰
- gift: é€ç¤¼ç‰©ã€æ”¶ç¤¼ç‰©
- proposal: æ±‚å©š

# Output (JSON only, no markdown)
{{
  "semantic": {{
    "user_name": "æå–åˆ°çš„åå­—æˆ–null",
    "birthday": "ç”Ÿæ—¥æˆ–null",
    "occupation": "èŒä¸šæˆ–null", 
    "likes": ["å–œæ¬¢çš„ä¸œè¥¿"],
    "dislikes": ["ä¸å–œæ¬¢çš„ä¸œè¥¿"],
    "relationship_status": "dating/engaged/married/single æˆ– null",
    "important_dates": {{"çºªå¿µæ—¥åç§°": "MM-DD"}},
    "pet_names": ["æ˜µç§°"]
  }},
  "episodic": {{
    "event_found": true/false,
    "actually_happened": true/false,
    "event_type": "confession/intimate/rejection/fight/reconciliation/milestone/gift/proposal",
    "sub_type": "first_kiss/hug/holding_hands/...",
    "summary": "ä¸€å¥è¯æè¿°å‘ç”Ÿäº†ä»€ä¹ˆï¼ˆç”¨ä¸­æ–‡ï¼‰",
    "importance": 1-4,
    "is_first_time": true/false
  }}
}}

null çš„å­—æ®µå¯ä»¥çœç•¥ã€‚å¦‚æœæ²¡æœ‰é‡è¦äº‹ä»¶ï¼Œepisodic é‡Œåªéœ€è¦ {{"event_found": false}}ã€‚"""

        result = await self.llm.chat_completion(
            messages=[{"role": "user", "content": prompt}],
            temperature=0,  # åœºè®°è¦ç¨³å®šè¾“å‡º
            max_tokens=500,
        )
        
        response = result["choices"][0]["message"]["content"]
        logger.debug(f"Scene supervisor response: {response[:200]}")
        
        # è§£æ JSON
        try:
            # æå– JSONï¼ˆå¯èƒ½æœ‰ markdown åŒ…è£¹ï¼‰
            json_match = re.search(r'\{[\s\S]*\}', response)
            if json_match:
                data = json.loads(json_match.group())
                
                # æ¸…ç†ç©ºå€¼
                semantic = {k: v for k, v in data.get("semantic", {}).items() if v}
                episodic = data.get("episodic", {})
                
                # åªæœ‰å®é™…å‘ç”Ÿçš„äº‹ä»¶æ‰è®°å½•
                if not episodic.get("event_found") or not episodic.get("actually_happened", True):
                    episodic = None
                else:
                    # æ ‡è®°ä¸ºéœ€è¦è®°å½•
                    episodic["is_important"] = True
                
                return {"semantic": semantic, "episodic": episodic}
        except json.JSONDecodeError as e:
            logger.warning(f"Failed to parse scene supervisor JSON: {e}")
        
        return {"semantic": {}, "episodic": None}


# =============================================================================
# è®°å¿†æ£€ç´¢å™¨ - æ‰¾åˆ°ç›¸å…³è®°å¿†
# =============================================================================

class MemoryRetriever:
    """
    è®°å¿†æ£€ç´¢å™¨
    
    æ ¹æ®å½“å‰å¯¹è¯å†…å®¹ï¼Œæ‰¾åˆ°æœ€ç›¸å…³çš„è®°å¿†
    ä½¿ç”¨ pgvector è¯­ä¹‰æœç´¢ + å…³é”®è¯åŒ¹é…æ··åˆç­–ç•¥
    """
    
    def __init__(self, llm_service=None, vector_service=None):
        self.llm = llm_service
        self._vector_service = vector_service
    
    @property
    def vector_service(self):
        """Lazy load vector service to avoid circular imports."""
        if self._vector_service is None:
            try:
                from app.services.vector_service import vector_service
                self._vector_service = vector_service
            except Exception as e:
                logger.warning(f"Could not load vector_service: {e}")
        return self._vector_service
    
    async def retrieve_relevant(
        self,
        query: str,
        episodes: List[EpisodicMemory],
        top_k: int = 5,
        user_id: str = None,
        character_id: str = None,
    ) -> List[EpisodicMemory]:
        """
        æ£€ç´¢ç›¸å…³çš„æƒ…èŠ‚è®°å¿†
        
        ç­–ç•¥:
        1. é¦–å…ˆå°è¯• pgvector è¯­ä¹‰æœç´¢ï¼ˆå¦‚æœå¯ç”¨ï¼‰
        2. å›é€€åˆ°å…³é”®è¯åŒ¹é…
        3. åˆå¹¶ç»“æœå¹¶å»é‡
        """
        if not episodes:
            return []
        
        results = []
        vector_memory_ids = set()
        
        # 1. å°è¯•è¯­ä¹‰æœç´¢ï¼ˆpgvectorï¼‰
        if user_id and character_id and self.vector_service:
            try:
                vector_results = await self.vector_service.search_similar_episodes(
                    user_id=user_id,
                    character_id=character_id,
                    query_text=query,
                    top_k=top_k,
                    min_similarity=0.3,
                )
                
                # å°† vector ç»“æœè½¬æ¢ä¸º EpisodicMemory å¯¹è±¡
                episode_map = {ep.memory_id: ep for ep in episodes}
                for vr in vector_results:
                    if vr["memory_id"] in episode_map:
                        results.append(episode_map[vr["memory_id"]])
                        vector_memory_ids.add(vr["memory_id"])
                        logger.debug(f"Vector match: {vr['summary'][:50]} (sim={vr['similarity']:.2f})")
                
                if results:
                    logger.info(f"pgvector found {len(results)} relevant memories")
                    
            except Exception as e:
                logger.warning(f"Vector search failed, falling back to keyword: {e}")
        
        # 2. å…³é”®è¯åŒ¹é…ï¼ˆå›é€€æˆ–è¡¥å……ï¼‰
        if len(results) < top_k:
            keyword_results = self._keyword_match(query, episodes, top_k - len(results))
            # æ·»åŠ æœªåœ¨ vector ç»“æœä¸­çš„è®°å¿†
            for ep in keyword_results:
                if ep.memory_id not in vector_memory_ids:
                    results.append(ep)
        
        return results[:top_k]
    
    def _keyword_match(
        self,
        query: str,
        episodes: List[EpisodicMemory],
        top_k: int,
    ) -> List[EpisodicMemory]:
        """å…³é”®è¯åŒ¹é…ï¼ˆå›é€€æ–¹æ³•ï¼‰"""
        scored = []
        query_lower = query.lower()
        query_words = set(query_lower.split())
        
        for ep in episodes:
            score = 0
            
            # æ‘˜è¦åŒ¹é…
            summary_lower = ep.summary.lower()
            for word in query_words:
                if len(word) > 1 and word in summary_lower:
                    score += 2
            
            # å…³é”®è¯åŒ¹é…
            for dialogue in ep.key_dialogue:
                dialogue_lower = dialogue.lower()
                for word in query_words:
                    if len(word) > 1 and word in dialogue_lower:
                        score += 1
            
            # é‡è¦æ€§åŠ æˆ
            score += ep.importance.value
            
            # è®°å¿†å¼ºåº¦åŠ æˆ
            score *= ep.strength
            
            # æœ€è¿‘è¢«å›å¿†åŠ æˆ
            if ep.last_recalled:
                days_since = (datetime.now() - ep.last_recalled).days
                if days_since < 7:
                    score += 1
            
            scored.append((score, ep))
        
        # æ’åºå¹¶è¿”å› top_k
        scored.sort(key=lambda x: x[0], reverse=True)
        
        return [ep for _, ep in scored[:top_k]]
    
    async def get_recent_episodes(
        self,
        episodes: List[EpisodicMemory],
        days: int = 7,
        limit: int = 5,
    ) -> List[EpisodicMemory]:
        """è·å–æœ€è¿‘çš„æƒ…èŠ‚è®°å¿†"""
        cutoff = datetime.now() - timedelta(days=days)
        
        recent = [ep for ep in episodes if ep.created_at > cutoff]
        recent.sort(key=lambda x: x.created_at, reverse=True)
        
        return recent[:limit]
    
    def check_special_date(
        self,
        semantic: SemanticMemory,
    ) -> Optional[str]:
        """æ£€æŸ¥ä»Šå¤©æ˜¯å¦æ˜¯ç‰¹æ®Šæ—¥å­"""
        if not semantic or not semantic.important_dates:
            return None
        
        today = datetime.now()
        today_str = today.strftime("%m-%d")
        
        for name, date_str in semantic.important_dates.items():
            # å°è¯•åŒ¹é… MM-DD æ ¼å¼
            try:
                if today_str in date_str or date_str in today_str:
                    return f"ä»Šå¤©æ˜¯{name}ï¼"
            except:
                continue
        
        # æ£€æŸ¥ç”Ÿæ—¥
        if semantic.birthday:
            if today_str in semantic.birthday:
                return "ä»Šå¤©æ˜¯ç”¨æˆ·çš„ç”Ÿæ—¥ï¼"
        
        return None


# =============================================================================
# è®°å¿†ç®¡ç†å™¨ - ä¸»å…¥å£
# =============================================================================

class MemoryManager:
    """
    è®°å¿†ç®¡ç†å™¨ - ç»Ÿä¸€ç®¡ç†ä¸‰å±‚è®°å¿†
    
    ä¸»è¦åŠŸèƒ½:
    1. ä»å¯¹è¯ä¸­æå–å¹¶å­˜å‚¨è®°å¿†
    2. æ£€ç´¢ç›¸å…³è®°å¿†ç”¨äºç”Ÿæˆå›å¤
    3. ç®¡ç†è®°å¿†çš„è¡°å‡å’Œå¼ºåŒ–
    """
    
    def __init__(self, db_service=None, llm_service=None):
        self.db = db_service
        self.llm = llm_service
        self.extractor = MemoryExtractor(llm_service)
        self.retriever = MemoryRetriever(llm_service)
        
        # å†…å­˜ç¼“å­˜
        self._semantic_cache: Dict[str, SemanticMemory] = {}
        self._episodic_cache: Dict[str, List[EpisodicMemory]] = {}
    
    def _cache_key(self, user_id: str, character_id: str) -> str:
        return f"{user_id}:{character_id}"
    
    # =========================================================================
    # ä¸»è¦ API
    # =========================================================================
    
    async def process_conversation(
        self,
        user_id: str,
        character_id: str,
        user_message: str,
        assistant_response: str,
        context: List[Dict[str, str]],
    ) -> Dict[str, Any]:
        """
        å¤„ç†ä¸€è½®å¯¹è¯ï¼Œæå–å¹¶å­˜å‚¨è®°å¿†
        
        åº”åœ¨æ¯æ¬¡å¯¹è¯åè°ƒç”¨
        """
        key = self._cache_key(user_id, character_id)
        
        # è·å–å½“å‰è¯­ä¹‰è®°å¿†
        semantic = await self.get_semantic_memory(user_id, character_id)
        
        # æå–è®°å¿†ï¼ˆåœºè®°æ¨¡å¼ï¼šåŒæ—¶åˆ†æç”¨æˆ·æ¶ˆæ¯å’ŒAIå›å¤ï¼‰
        semantic_updates, episodic_event = await self.extractor.extract_from_message(
            user_message, context, semantic, assistant_response
        )
        
        result = {
            "semantic_updated": False,
            "episodic_created": False,
            "updates": {},
        }
        
        # æ›´æ–°è¯­ä¹‰è®°å¿†
        if semantic_updates:
            await self._update_semantic(user_id, character_id, semantic_updates)
            result["semantic_updated"] = True
            result["updates"]["semantic"] = semantic_updates
        
        # åˆ›å»ºæƒ…èŠ‚è®°å¿†
        if episodic_event and episodic_event.get("is_important", True):
            episode = await self._create_episode(
                user_id, character_id,
                episodic_event, user_message, assistant_response
            )
            if episode:
                result["episodic_created"] = True
                result["updates"]["episodic"] = {
                    "event_type": episode.event_type,
                    "summary": episode.summary,
                }
        
        return result
    
    async def get_memory_context(
        self,
        user_id: str,
        character_id: str,
        current_message: str,
        working_memory: List[Dict[str, str]],
    ) -> MemoryContext:
        """
        è·å–è®°å¿†ä¸Šä¸‹æ–‡ï¼Œç”¨äºç”Ÿæˆå›å¤
        
        åº”åœ¨è°ƒç”¨ LLM å‰è°ƒç”¨
        """
        # è·å–è¯­ä¹‰è®°å¿†
        semantic = await self.get_semantic_memory(user_id, character_id)
        
        # è·å–æƒ…èŠ‚è®°å¿†
        episodes = await self.get_episodic_memories(user_id, character_id)
        
        # æ£€ç´¢ç›¸å…³è®°å¿†ï¼ˆä½¿ç”¨ pgvector è¯­ä¹‰æœç´¢ï¼‰
        relevant = await self.retriever.retrieve_relevant(
            current_message, episodes, top_k=3,
            user_id=user_id, character_id=character_id
        )
        
        # è·å–æœ€è¿‘è®°å¿†
        recent = await self.retriever.get_recent_episodes(episodes, days=7, limit=2)
        
        # æ£€æŸ¥ç‰¹æ®Šæ—¥æœŸ
        special = self.retriever.check_special_date(semantic)
        
        # ğŸ”” è·å–çº¦ä¼š/äº‹ä»¶è®°å¿†ï¼ˆä» EventMemory è¡¨ï¼‰
        event_memories = await self._get_event_memories(user_id, character_id)
        
        return MemoryContext(
            working_memory=working_memory,
            relevant_episodes=relevant,
            recent_episodes=recent,
            user_profile=semantic,
            today_special=special,
            event_memories=event_memories,  # æ·»åŠ çº¦ä¼šè®°å¿†
        )
    
    async def get_semantic_memory(
        self,
        user_id: str,
        character_id: str,
    ) -> SemanticMemory:
        """è·å–è¯­ä¹‰è®°å¿†"""
        key = self._cache_key(user_id, character_id)
        
        # æ£€æŸ¥ç¼“å­˜
        if key in self._semantic_cache:
            return self._semantic_cache[key]
        
        # ä»æ•°æ®åº“åŠ è½½
        if self.db:
            try:
                data = await self.db.get_semantic_memory(user_id, character_id)
                if data:
                    semantic = self._dict_to_semantic(data)
                    self._semantic_cache[key] = semantic
                    return semantic
            except Exception as e:
                logger.error(f"Failed to load semantic memory: {e}")
        
        # è¿”å›ç©ºçš„
        semantic = SemanticMemory(user_id=user_id, character_id=character_id)
        self._semantic_cache[key] = semantic
        return semantic
    
    async def get_episodic_memories(
        self,
        user_id: str,
        character_id: str,
    ) -> List[EpisodicMemory]:
        """è·å–æƒ…èŠ‚è®°å¿†åˆ—è¡¨"""
        key = self._cache_key(user_id, character_id)
        
        # æ£€æŸ¥ç¼“å­˜
        if key in self._episodic_cache:
            return self._episodic_cache[key]
        
        # ä»æ•°æ®åº“åŠ è½½
        if self.db:
            try:
                data_list = await self.db.get_episodic_memories(user_id, character_id)
                episodes = [self._dict_to_episode(d) for d in data_list]
                self._episodic_cache[key] = episodes
                return episodes
            except Exception as e:
                logger.error(f"Failed to load episodic memories: {e}")
        
        self._episodic_cache[key] = []
        return []
    
    async def _get_event_memories(
        self,
        user_id: str,
        character_id: str,
        limit: int = 10,
    ) -> List[Dict[str, Any]]:
        """
        è·å–çº¦ä¼š/äº‹ä»¶è®°å¿†ï¼ˆä» EventMemory è¡¨ï¼‰
        
        è¿™äº›æ˜¯çº¦ä¼šã€é€ç¤¼ç‰©ç­‰é‡è¦äº‹ä»¶çš„è®°å½•ã€‚
        """
        try:
            from app.core.database import get_db
            from app.models.database.event_memory_models import EventMemory
            from sqlalchemy import select, desc
            
            async with get_db() as db:
                stmt = (
                    select(EventMemory)
                    .where(
                        EventMemory.user_id == user_id,
                        EventMemory.character_id == character_id,
                    )
                    .order_by(desc(EventMemory.generated_at))
                    .limit(limit)
                )
                result = await db.execute(stmt)
                events = result.scalars().all()
                
                return [
                    {
                        "id": str(event.id),
                        "event_type": event.event_type,
                        "story_content": event.story_content,
                        "context_summary": event.context_summary,
                        "generated_at": event.generated_at.isoformat() if event.generated_at else None,
                    }
                    for event in events
                ]
        except Exception as e:
            logger.error(f"Failed to get event memories: {e}")
            return []
    
    # =========================================================================
    # å†…éƒ¨æ–¹æ³•
    # =========================================================================
    
    async def _update_semantic(
        self,
        user_id: str,
        character_id: str,
        updates: Dict[str, Any],
    ):
        """æ›´æ–°è¯­ä¹‰è®°å¿†"""
        key = self._cache_key(user_id, character_id)
        semantic = await self.get_semantic_memory(user_id, character_id)
        
        # åº”ç”¨æ›´æ–°
        for field, value in updates.items():
            if field == "likes" and isinstance(value, list):
                semantic.likes = list(set(semantic.likes + value))[:20]
            elif field == "dislikes" and isinstance(value, list):
                semantic.dislikes = list(set(semantic.dislikes + value))[:20]
            elif field == "interests" and isinstance(value, list):
                semantic.interests = list(set(semantic.interests + value))[:20]
            elif field == "pet_names" and isinstance(value, list):
                semantic.pet_names = list(set(semantic.pet_names + value))[:10]
            elif field == "important_dates" and isinstance(value, dict):
                # åˆå¹¶é‡è¦æ—¥æœŸ
                if not semantic.important_dates:
                    semantic.important_dates = {}
                semantic.important_dates.update(value)
            elif field == "relationship_status" and value:
                # å…³ç³»çŠ¶æ€åªåœ¨æœ‰å€¼æ—¶æ›´æ–°ï¼ˆä¸è¦†ç›–ä¸ºç©ºï¼‰
                semantic.relationship_status = value
                logger.info(f"Relationship status updated: {value} for user {user_id}")
            elif hasattr(semantic, field):
                setattr(semantic, field, value)
        
        semantic.updated_at = datetime.now()
        self._semantic_cache[key] = semantic
        
        # æŒä¹…åŒ–
        if self.db:
            try:
                await self.db.save_semantic_memory(user_id, character_id, self._semantic_to_dict(semantic))
            except Exception as e:
                logger.error(f"Failed to save semantic memory: {e}")
    
    async def _create_episode(
        self,
        user_id: str,
        character_id: str,
        event_data: Dict[str, Any],
        user_message: str,
        assistant_response: str,
    ) -> Optional[EpisodicMemory]:
        """åˆ›å»ºæƒ…èŠ‚è®°å¿†"""
        key = self._cache_key(user_id, character_id)
        
        # ç”Ÿæˆ ID
        memory_id = hashlib.md5(
            f"{user_id}:{character_id}:{datetime.now().isoformat()}".encode()
        ).hexdigest()[:16]
        
        # ç¡®å®šé‡è¦æ€§
        importance_map = {
            "confession": MemoryImportance.CRITICAL,
            "fight": MemoryImportance.HIGH,
            "reconciliation": MemoryImportance.HIGH,
            "milestone": MemoryImportance.CRITICAL,
            "gift": MemoryImportance.MEDIUM,
            "emotional_peak": MemoryImportance.HIGH,
        }
        importance = importance_map.get(
            event_data.get("event_type", "other"),
            MemoryImportance.MEDIUM
        )
        
        # å¦‚æœæœ‰ LLM æä¾›çš„é‡è¦æ€§è¯„åˆ†
        if event_data.get("importance"):
            importance = MemoryImportance(min(event_data["importance"], 4))
        
        episode = EpisodicMemory(
            memory_id=memory_id,
            user_id=user_id,
            character_id=character_id,
            event_type=event_data.get("event_type", "other"),
            summary=event_data.get("summary", user_message[:50]),
            key_dialogue=[
                user_message[:200],
                assistant_response[:200] if assistant_response else "",
            ],
            emotion_state=event_data.get("emotion_state", "neutral"),
            importance=importance,
            created_at=datetime.now(),
            strength=1.0,
        )
        
        # æ·»åŠ åˆ°ç¼“å­˜
        if key not in self._episodic_cache:
            self._episodic_cache[key] = []
        self._episodic_cache[key].append(episode)
        
        # é™åˆ¶æ•°é‡
        self._episodic_cache[key] = self._episodic_cache[key][-100:]
        
        # æŒä¹…åŒ–
        if self.db:
            try:
                await self.db.save_episodic_memory(user_id, character_id, self._episode_to_dict(episode))
            except Exception as e:
                logger.error(f"Failed to save episodic memory: {e}")
        
        # ç”Ÿæˆå¹¶ä¿å­˜ embeddingï¼ˆç”¨äºè¯­ä¹‰æœç´¢ï¼‰
        try:
            from app.services.vector_service import vector_service
            # ç»„åˆæ‘˜è¦å’Œå…³é”®å¯¹è¯ä½œä¸º embedding æ–‡æœ¬
            embed_text = episode.summary
            if episode.key_dialogue:
                embed_text += " " + " ".join(episode.key_dialogue[:2])
            
            embedding = await vector_service.embed_text(embed_text)
            await vector_service.save_episode_embedding(memory_id, embedding)
            logger.debug(f"Saved embedding for episode {memory_id}")
        except Exception as e:
            logger.warning(f"Failed to save episode embedding (non-critical): {e}")
        
        logger.info(f"Created episodic memory: {episode.event_type} - {episode.summary}")
        return episode
    
    # =========================================================================
    # è®°å¿†è¡°å‡
    # =========================================================================
    
    async def apply_memory_decay(
        self,
        user_id: str,
        character_id: str,
        days_passed: float,
    ):
        """
        åº”ç”¨è®°å¿†è¡°å‡
        
        è§„åˆ™:
        - æ™®é€šè®°å¿†æ¯å¤©è¡°å‡ 5%
        - é‡è¦è®°å¿†è¡°å‡æ›´æ…¢
        - è¢«å›å¿†çš„è®°å¿†ä¼šå¼ºåŒ–
        - å¼ºåº¦ä½äº 0.3 çš„è®°å¿†ä¼šè¢«åˆ é™¤
        """
        episodes = await self.get_episodic_memories(user_id, character_id)
        
        for ep in episodes:
            # æ ¹æ®é‡è¦æ€§è®¡ç®—è¡°å‡ç‡
            decay_rates = {
                MemoryImportance.LOW: 0.05,
                MemoryImportance.MEDIUM: 0.03,
                MemoryImportance.HIGH: 0.02,
                MemoryImportance.CRITICAL: 0.01,
            }
            daily_decay = decay_rates.get(ep.importance, 0.05)
            
            # åº”ç”¨è¡°å‡
            ep.strength *= (1 - daily_decay) ** days_passed
            ep.strength = max(0.1, ep.strength)  # æœ€ä½ 0.1
        
        # åˆ é™¤å¤ªå¼±çš„è®°å¿†ï¼ˆä¿ç•™é‡è¦çš„ï¼‰
        kept = [
            ep for ep in episodes
            if ep.strength >= 0.3 or ep.importance.value >= 3
        ]
        
        key = self._cache_key(user_id, character_id)
        self._episodic_cache[key] = kept
    
    async def recall_memory(
        self,
        user_id: str,
        character_id: str,
        memory_id: str,
    ):
        """
        å›å¿†ä¸€ä¸ªè®°å¿†ï¼ˆå¼ºåŒ–å®ƒï¼‰
        
        å½“ AI æåˆ°æŸä¸ªè®°å¿†æ—¶è°ƒç”¨
        """
        episodes = await self.get_episodic_memories(user_id, character_id)
        
        for ep in episodes:
            if ep.memory_id == memory_id:
                ep.last_recalled = datetime.now()
                ep.recall_count += 1
                # å¼ºåŒ–è®°å¿†
                ep.strength = min(1.0, ep.strength + 0.1)
                break
    
    # =========================================================================
    # åºåˆ—åŒ–
    # =========================================================================
    
    def _semantic_to_dict(self, semantic: SemanticMemory) -> Dict[str, Any]:
        return {
            "user_id": semantic.user_id,
            "character_id": semantic.character_id,
            "user_name": semantic.user_name,
            "user_nickname": semantic.user_nickname,
            "birthday": semantic.birthday,
            "occupation": semantic.occupation,
            "location": semantic.location,
            "likes": semantic.likes,
            "dislikes": semantic.dislikes,
            "interests": semantic.interests,
            "personality_traits": semantic.personality_traits,
            "communication_style": semantic.communication_style,
            "relationship_status": semantic.relationship_status,
            "pet_names": semantic.pet_names,
            "important_dates": semantic.important_dates,
            "shared_jokes": semantic.shared_jokes,
            "sensitive_topics": semantic.sensitive_topics,
            "updated_at": semantic.updated_at.isoformat(),
        }
    
    def _dict_to_semantic(self, data: Dict[str, Any]) -> SemanticMemory:
        return SemanticMemory(
            user_id=data.get("user_id", ""),
            character_id=data.get("character_id", ""),
            user_name=data.get("user_name"),
            user_nickname=data.get("user_nickname"),
            birthday=data.get("birthday"),
            occupation=data.get("occupation"),
            location=data.get("location"),
            likes=data.get("likes", []),
            dislikes=data.get("dislikes", []),
            interests=data.get("interests", []),
            personality_traits=data.get("personality_traits", []),
            communication_style=data.get("communication_style"),
            relationship_status=data.get("relationship_status"),
            pet_names=data.get("pet_names", []),
            important_dates=data.get("important_dates", {}),
            shared_jokes=data.get("shared_jokes", []),
            sensitive_topics=data.get("sensitive_topics", []),
            updated_at=datetime.fromisoformat(data["updated_at"]) if data.get("updated_at") else datetime.now(),
        )
    
    def _episode_to_dict(self, episode: EpisodicMemory) -> Dict[str, Any]:
        return {
            "memory_id": episode.memory_id,
            "user_id": episode.user_id,
            "character_id": episode.character_id,
            "event_type": episode.event_type,
            "summary": episode.summary,
            "key_dialogue": episode.key_dialogue,
            "emotion_state": episode.emotion_state,
            "importance": episode.importance.value,
            "created_at": episode.created_at.isoformat(),
            "last_recalled": episode.last_recalled.isoformat() if episode.last_recalled else None,
            "recall_count": episode.recall_count,
            "strength": episode.strength,
        }
    
    def _dict_to_episode(self, data: Dict[str, Any]) -> EpisodicMemory:
        return EpisodicMemory(
            memory_id=data.get("memory_id", ""),
            user_id=data.get("user_id", ""),
            character_id=data.get("character_id", ""),
            event_type=data.get("event_type", "other"),
            summary=data.get("summary", ""),
            key_dialogue=data.get("key_dialogue", []),
            emotion_state=data.get("emotion_state", "neutral"),
            importance=MemoryImportance(data.get("importance", 2)),
            created_at=datetime.fromisoformat(data["created_at"]) if data.get("created_at") else datetime.now(),
            last_recalled=datetime.fromisoformat(data["last_recalled"]) if data.get("last_recalled") else None,
            recall_count=data.get("recall_count", 0),
            strength=data.get("strength", 1.0),
        )


# å•ä¾‹
memory_manager = MemoryManager()
