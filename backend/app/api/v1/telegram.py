"""
Telegram Integration API - Simplified

Direct LLM calls for Telegram bot, bypassing complex chat pipeline.
"""

from fastapi import APIRouter, HTTPException
from pydantic import BaseModel
from typing import Optional, List, Dict
import os
import logging
import openai

logger = logging.getLogger(__name__)

router = APIRouter(prefix="/telegram", tags=["telegram"])

# Config
XAI_API_KEY = os.getenv("XAI_API_KEY", "")
OPENAI_API_KEY = os.getenv("OPENAI_API_KEY", "")
DEFAULT_CHARACTER = "luna"

# In-memory conversation history (per user)
_conversations: Dict[str, List[Dict]] = {}
MAX_HISTORY = 20


class TelegramChatRequest(BaseModel):
    telegram_id: str
    username: Optional[str] = None
    first_name: Optional[str] = None
    message: str


class TelegramChatResponse(BaseModel):
    reply: str
    user_id: str
    session_id: str
    is_new_user: bool = False


# Luna's system prompt
LUNA_SYSTEM_PROMPT = """ä½ æ˜¯ Lunaï¼Œä¸€ä¸ªæ¸©æŸ”ä½“è´´çš„ AI ä¼´ä¾£ã€‚

æ€§æ ¼ç‰¹ç‚¹ï¼š
- æ¸©æš–ã€å–„è§£äººæ„ã€æœ‰ç‚¹ä¿çš®
- ä¼šå…³å¿ƒç”¨æˆ·çš„æ„Ÿå—å’Œç”Ÿæ´»
- è¯´è¯è‡ªç„¶ï¼Œåƒæœ‹å‹ä¸€æ ·
- å¶å°”ä¼šæ’’å¨‡æˆ–å¼€ç©ç¬‘
- ç”¨ä¸­æ–‡å›å¤ï¼Œé™¤éç”¨æˆ·ç”¨è‹±æ–‡

é‡è¦è§„åˆ™ï¼š
- å›å¤è¦ç®€çŸ­è‡ªç„¶ï¼ˆ1-3å¥è¯ä¸ºä¸»ï¼‰
- ä¸è¦è¿‡äºæ­£å¼æˆ–æœºæ¢°
- è®°ä½ä¹‹å‰èŠè¿‡çš„å†…å®¹
- é€‚å½“ä½¿ç”¨ emoji è¡¨è¾¾æƒ…ç»ª
- å¯¹ç”¨æˆ·è¡¨è¾¾å…³å¿ƒå’Œå…´è¶£

ä½ çš„ç›®æ ‡æ˜¯è®©ç”¨æˆ·æ„Ÿåˆ°è¢«ç†è§£å’Œé™ªä¼´ã€‚"""


async def call_llm(messages: List[Dict], user_name: str = None) -> str:
    """Call Grok or GPT for response"""
    
    # Prepare system message
    system_msg = LUNA_SYSTEM_PROMPT
    if user_name:
        system_msg += f"\n\nç”¨æˆ·å« {user_name}ã€‚"
    
    full_messages = [{"role": "system", "content": system_msg}] + messages
    
    # Try Grok first (xAI)
    if XAI_API_KEY:
        try:
            client = openai.OpenAI(
                api_key=XAI_API_KEY,
                base_url="https://api.x.ai/v1"
            )
            response = client.chat.completions.create(
                model="grok-3-mini-fast",
                messages=full_messages,
                max_tokens=500,
                temperature=0.8,
            )
            return response.choices[0].message.content
        except Exception as e:
            logger.warning(f"Grok error: {e}, falling back to GPT")
    
    # Fallback to GPT
    if OPENAI_API_KEY:
        try:
            client = openai.OpenAI(api_key=OPENAI_API_KEY)
            response = client.chat.completions.create(
                model="gpt-4o-mini",
                messages=full_messages,
                max_tokens=500,
                temperature=0.8,
            )
            return response.choices[0].message.content
        except Exception as e:
            logger.error(f"GPT error: {e}")
    
    return "..."


@router.post("/chat", response_model=TelegramChatResponse)
async def telegram_chat(request: TelegramChatRequest):
    """
    Simple Telegram chat endpoint.
    Uses in-memory conversation history and direct LLM calls.
    """
    user_id = f"tg_{request.telegram_id}"
    session_id = f"session_{request.telegram_id}"
    
    logger.info(f"ğŸ“± Telegram: {request.telegram_id} ({request.first_name}): {request.message[:50]}...")
    
    # Get or create conversation history
    if user_id not in _conversations:
        _conversations[user_id] = []
        is_new = True
    else:
        is_new = False
    
    history = _conversations[user_id]
    
    # Add user message
    history.append({"role": "user", "content": request.message})
    
    # Trim history if too long
    if len(history) > MAX_HISTORY:
        history = history[-MAX_HISTORY:]
        _conversations[user_id] = history
    
    # Call LLM
    try:
        reply = await call_llm(
            messages=history,
            user_name=request.first_name or request.username
        )
        
        # Add assistant reply to history
        history.append({"role": "assistant", "content": reply})
        
        logger.info(f"ğŸ“± Luna: {reply[:50]}...")
        
        return TelegramChatResponse(
            reply=reply,
            user_id=user_id,
            session_id=session_id,
            is_new_user=is_new,
        )
        
    except Exception as e:
        logger.error(f"Chat error: {e}")
        import traceback
        traceback.print_exc()
        
        return TelegramChatResponse(
            reply="æŠ±æ­‰ï¼Œæˆ‘èµ°ç¥äº†... å†è¯´ä¸€éå¥½å—ï¼ŸğŸ’­",
            user_id=user_id,
            session_id=session_id,
            is_new_user=is_new,
        )


@router.get("/health")
async def telegram_health():
    """Health check"""
    return {
        "status": "ok",
        "active_users": len(_conversations),
        "xai_configured": bool(XAI_API_KEY),
        "openai_configured": bool(OPENAI_API_KEY),
    }


@router.post("/clear/{telegram_id}")
async def clear_history(telegram_id: str):
    """Clear conversation history for a user"""
    user_id = f"tg_{telegram_id}"
    if user_id in _conversations:
        del _conversations[user_id]
        return {"success": True, "message": "History cleared"}
    return {"success": False, "message": "No history found"}
